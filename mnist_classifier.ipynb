{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier\n",
    "Training a basic feedforward neural network to classify handwritten\n",
    "digits from the MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few of the images\n",
    "fig, ax = plt.subplots(1,3)\n",
    "for i in range(3):\n",
    "    ax[i].imshow(example_data[i].squeeze(), cmap='gray')\n",
    "    ax[i].set(title=f\"{example_targets[i]}\")\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28x28 pixel images\n",
    "hidden_size = 196 # 196\n",
    "num_classes = 10 # 10 classes\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "count = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Do a forward pass to get a prediction from the model\n",
    "    pred = model(data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(pred, target)\n",
    "\n",
    "    if count > 10:\n",
    "        loss_val = loss.detach().item()\n",
    "        print(loss_val)\n",
    "        loss_hist.append(loss_val)\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "    # Compute gradients and take a gradient step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "three = example_data[0]\n",
    "nine = example_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0d9094fd30>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.imshow(three.squeeze(), cmap=\"gray\")\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.imshow(nine.squeeze(), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.0 %\n",
      "1: 0.0 %\n",
      "2: 0.0 %\n",
      "3: 0.4999999888241291 %\n",
      "4: 2.4000000208616257 %\n",
      "5: 0.4000000189989805 %\n",
      "6: 0.0 %\n",
      "7: 2.800000086426735 %\n",
      "8: 0.4000000189989805 %\n",
      "9: 93.50000023841858 %\n"
     ]
    }
   ],
   "source": [
    "res = model(nine).detach()\n",
    "res = F.softmax(res, dim=1).numpy().flatten()\n",
    "res = np.round(res, 3)\n",
    "for i in range(len(res)):\n",
    "    print(f\"{i}: {100*res[i]} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
